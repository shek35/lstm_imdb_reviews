{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "7c3ab45d-c79f-465d-8041-78bf2a58173d",
      "metadata": {
        "id": "7c3ab45d-c79f-465d-8041-78bf2a58173d"
      },
      "source": [
        "Train an LSTM Model \n",
        "----\n",
        "1. Using PyTorch, implement a neural network that uses one or more LSTM cells to do sentiment analysis. Use the nn.Embedding, nn.LSTM and nn.Linear layers to construct your model.\n",
        "2. Note that sequence processing works differently with the PyTorch Embedding layer as compared to my sample code from class. The model input expects a padded tensor of token indices from the vocabulary, instead of one-hot encodings. For evaluation, use a vocabulary size of 10000 (max_features = 10000).\n",
        "3. The model should have a single output with the sigmoid activation function for classification. The dimensions of the embedding layer and the hidden layer(s) are up to you, but please make sure your model does not take more than ~3 minutes to train.\n",
        "4. Evaluate the model using PyTorch functions for average accuracy, area under the ROC curve and F1 scores (see [torchedev](https://pytorch.org/torcheval/stable/))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "04554ea0-360d-43cd-b5d4-67442da6dbb6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "04554ea0-360d-43cd-b5d4-67442da6dbb6",
        "outputId": "46dabd70-8a8b-4718-bfc5-a3cb72de48ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.10/dist-packages (1.3.2)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.25.2)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (24.0)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.2.1+cu121)\n",
            "Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (0.11.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (67.7.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.11.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.13.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->torchmetrics) (12.4.127)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->torchmetrics) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->torchmetrics) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "import string\n",
        "import re\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "!pip install torchmetrics\n",
        "from torchmetrics.functional import f1_score, auroc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "b01e664a-e80a-4129-968c-3a3df25617a5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b01e664a-e80a-4129-968c-3a3df25617a5",
        "outputId": "76f545ad-145c-4137-8936-df4d7b3a09a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU used\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"GPU is available\")\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "    print(\"MPS is available\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"CPU used\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "LkARJbsdGaP6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LkARJbsdGaP6",
        "outputId": "6d5dce7e-fbbf-4e3e-a878-e01bfd550495"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "2d028c44-0bcc-47a1-b9fe-78d1ddc030f2",
      "metadata": {
        "id": "2d028c44-0bcc-47a1-b9fe-78d1ddc030f2"
      },
      "outputs": [],
      "source": [
        "train_data_file = 'movie_reviews_train.txt'\n",
        "train_df = pd.read_csv(train_data_file, sep='\\t', header=None, names=['id', 'review', 'label'])[['review', 'label']]\n",
        "X_train, y_train = train_df['review'].values, train_df['label'].values\n",
        "\n",
        "dev_data_file = 'movie_reviews_dev.txt'\n",
        "dev_df = pd.read_csv(dev_data_file, sep='\\t', header=None, names=['id', 'review', 'label'])[['review', 'label']]\n",
        "X_dev, y_dev = dev_df['review'].values, dev_df['label'].values\n",
        "\n",
        "test_data_file = 'movie_reviews_test.txt'\n",
        "test_df = pd.read_csv(test_data_file, sep='\\t', header=None, names=['id', 'review', 'label'])[['review', 'label']]\n",
        "X_test, y_test = test_df['review'].values, test_df['label'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "fa0d2b64-25f0-4bdd-a6ff-8a8aea1c586b",
      "metadata": {
        "id": "fa0d2b64-25f0-4bdd-a6ff-8a8aea1c586b"
      },
      "outputs": [],
      "source": [
        "def preprocess_token(s): # This function is for pre-processing each token, not the entire sequence\n",
        "    # Retain only alphanumeric characters\n",
        "    s = re.sub(r'[^\\w\\s]+', '', s)\n",
        "\n",
        "    # replace digits with no space\n",
        "    s = re.sub(r'\\d', '', s)\n",
        "\n",
        "    # Replace all whitespace sequences with no space\n",
        "    s = re.sub(r'\\s+', '', s)\n",
        "\n",
        "    return s\n",
        "\n",
        "def tokenize(x_train, x_dev, x_test, vocab_size): # This function is for pre-processing strings, which uses the above.\n",
        "\n",
        "    # Remove stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    #Use this list to remove given stop words\n",
        "\n",
        "\n",
        "    # Retain the 'vocab_size' most frequent words\n",
        "    tokens = []\n",
        "    for doc in x_train:\n",
        "        for word in doc.split():\n",
        "            word = preprocess_token(word.lower())\n",
        "            if word not in stop_words and word != '':\n",
        "                tokens.append(word)\n",
        "\n",
        "    word_counts = Counter(tokens)\n",
        "    # vocab = [word for word, count in word_counts.most_common(vocab_size) if word not in stop_words]\n",
        "    vocab = sorted(word_counts, key=word_counts.get,reverse=True)[:vocab_size]\n",
        "    word_to_idx = {word: i+1 for i, word in enumerate(vocab)}\n",
        "\n",
        "    # Initialize empty lists to store padded sequences for training, development, and testing data\n",
        "    x_train_padded = []\n",
        "    x_dev_padded = []\n",
        "    x_test_padded = []\n",
        "\n",
        "    # Iterate through each document in the training data\n",
        "\n",
        "    for doc in x_train:\n",
        "\n",
        "        # Tokenize the document, convert tokens to lowercase, and preprocess each token\n",
        "        # Then, convert tokens to their corresponding indices in the vocabulary if they exist\n",
        "        tokens = [word_to_idx[preprocess_token(word.lower())] for word in doc.split() if preprocess_token(word.lower()) in word_to_idx.keys()]\n",
        "        x_train_padded.append(tokens)\n",
        "\n",
        "\n",
        "    # Iterate through each document in the development data\n",
        "\n",
        "    for doc in x_dev:\n",
        "\n",
        "        # Tokenize the document, convert tokens to lowercase, and preprocess each token\n",
        "        # Then, convert tokens to their corresponding indices in the vocabulary if they exist\n",
        "\n",
        "        tokens = [word_to_idx[preprocess_token(word.lower())] for word in doc.split() if preprocess_token(word.lower()) in word_to_idx.keys()]\n",
        "        x_dev_padded.append(tokens)\n",
        "\n",
        "    # Iterate through each document in the testing data\n",
        "\n",
        "    for doc in x_test:\n",
        "\n",
        "        # Tokenize the document, convert tokens to lowercase, and preprocess each token\n",
        "        # Then, convert tokens to their corresponding indices in the vocabulary if they exist\n",
        "\n",
        "        tokens = [word_to_idx[preprocess_token(word.lower())] for word in doc.split() if preprocess_token(word.lower()) in word_to_idx.keys()]\n",
        "        x_test_padded.append(tokens)\n",
        "\n",
        "    # Determine the maximum sequence size among all datasets (training, development, and testing)\n",
        "    max_seq_len = max([\n",
        "        max([len(seq) for seq in x_train_padded]),\n",
        "        max([len(seq) for seq in x_dev_padded]),\n",
        "        max([len(seq) for seq in x_test_padded])\n",
        "    ])\n",
        "    print(\"Max Seq Len:\", max_seq_len)\n",
        "\n",
        "    # Pad sequences in the training, testing and development data to ensure uniform length using zero-padding\n",
        "    train_padded = np.zeros((len(x_train_padded), max_seq_len),dtype=int)\n",
        "    for i, doc in enumerate(x_train_padded):\n",
        "        if len(doc) != 0:\n",
        "            train_padded[i, -len(doc):] = np.array(doc)[:max_seq_len]\n",
        "\n",
        "    dev_padded = np.zeros((len(x_dev_padded), max_seq_len),dtype=int)\n",
        "    for i, doc in enumerate(x_dev_padded):\n",
        "        if len(doc) != 0:\n",
        "            dev_padded[i, -len(doc):] = np.array(doc)[:max_seq_len]\n",
        "\n",
        "    test_padded = np.zeros((len(x_test_padded), max_seq_len),dtype=int)\n",
        "    for i, doc in enumerate(x_test_padded):\n",
        "        if len(doc) != 0:\n",
        "            test_padded[i, -len(doc):] = np.array(doc)[:max_seq_len]\n",
        "\n",
        "    # Finally, return the padded sequences (train, development and test) and vocabulary\n",
        "\n",
        "    return np.array(train_padded), np.array(dev_padded), np.array(test_padded), word_to_idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "912fd19f-04b5-4549-b944-142b99be21f7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "912fd19f-04b5-4549-b944-142b99be21f7",
        "outputId": "d579fa58-d816-43a1-b984-d6a3df6cb18f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max Seq Len: 608\n"
          ]
        }
      ],
      "source": [
        "# Tokenize your train, test and development data\n",
        "\n",
        "\n",
        "vocab_size = 10000\n",
        "\n",
        "X_train, X_dev, X_test, vocab = tokenize(X_train, X_dev, X_test, vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "ee92ad20-ceea-4579-ae72-03b4cb0d7b39",
      "metadata": {
        "id": "ee92ad20-ceea-4579-ae72-03b4cb0d7b39"
      },
      "outputs": [],
      "source": [
        "train_data = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n",
        "dev_data = TensorDataset(torch.from_numpy(X_dev), torch.from_numpy(y_dev))\n",
        "test_data = TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test))\n",
        "\n",
        "# dataloaders\n",
        "batch_size = 50\n",
        "\n",
        "# make sure to SHUFFLE your data\n",
        "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
        "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)\n",
        "dev_loader = DataLoader(dev_data, shuffle=True, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "0dea3832-6ad4-4c9c-845b-bfbaa04dc03a",
      "metadata": {
        "id": "0dea3832-6ad4-4c9c-845b-bfbaa04dc03a"
      },
      "outputs": [],
      "source": [
        "class SentimentRNN(nn.Module):\n",
        "    def __init__(self,num_layers,vocab_size,hidden_dim,embedding_dim,drop_prob=0.5):\n",
        "        super(SentimentRNN,self).__init__()\n",
        "\n",
        "\n",
        "        # embedding and LSTM layers\n",
        "\n",
        "        self.embedding_layer = nn.Embedding(vocab_size,embedding_dim)\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.no_layers = num_layers\n",
        "\n",
        "        # lstm\n",
        "\n",
        "        self.lstm = nn.LSTM(embedding_dim,hidden_dim,num_layers,dropout=drop_prob,batch_first=True)\n",
        "\n",
        "        # dropout layer\n",
        "\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "\n",
        "\n",
        "        # linear and sigmoid layer\n",
        "        self.fc = nn.Linear(hidden_dim,1)\n",
        "        self.sig = nn.Sigmoid()\n",
        "\n",
        "\n",
        "    def forward(self,x,hidden):\n",
        "\n",
        "        # embeddings and lstm_out\n",
        "        embeds = self.embedding_layer(x)\n",
        "        lstm_out,hidden = self.lstm(embeds,hidden)\n",
        "\n",
        "        # stack up lstm outputs\n",
        "        lstm_out = lstm_out[:,-1,:]\n",
        "\n",
        "        # dropout and fully connected layer\n",
        "        out = self.dropout(lstm_out)\n",
        "        out = self.fc(out)\n",
        "\n",
        "        # sigmoid function\n",
        "        sig_out = self.sig(out)\n",
        "\n",
        "        return sig_out,hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        ''' Initializes hidden state '''\n",
        "        # initialize hidden state(s) and cell state(s) of LSTM to zero with appropriate dimensions\n",
        "        h0 = torch.zeros((self.no_layers,batch_size,self.hidden_dim)).to(device)\n",
        "        c0 = torch.zeros((self.no_layers,batch_size,self.hidden_dim)).to(device)\n",
        "        hidden = (h0,c0)\n",
        "        return hidden\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "76fbf614-ddc3-475a-a488-a8dddad7282b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76fbf614-ddc3-475a-a488-a8dddad7282b",
        "outputId": "73072637-4c44-476d-edef-09b1c3ae64cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SentimentRNN(\n",
            "  (embedding_layer): Embedding(10001, 64)\n",
            "  (lstm): LSTM(64, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
            "  (sig): Sigmoid()\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "no_layers = 2\n",
        "vocab_size = len(vocab) + 1\n",
        "embedding_dim = 64\n",
        "output_dim = 1\n",
        "hidden_dim = 256\n",
        "\n",
        "model = SentimentRNN(no_layers,vocab_size,hidden_dim,embedding_dim,drop_prob=0.5)\n",
        "\n",
        "#moving to gpu\n",
        "model.to(device)\n",
        "\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "b8d96d5d-aa54-41e0-856f-03cd7a740029",
      "metadata": {
        "id": "b8d96d5d-aa54-41e0-856f-03cd7a740029"
      },
      "outputs": [],
      "source": [
        "lr=0.001\n",
        "\n",
        "# you should use binary cross-entropy as your loss function and Adam optimizer\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "# function to predict accuracy\n",
        "def acc(pred,label):\n",
        "    pred = torch.round(pred.squeeze())\n",
        "    return torch.sum(pred == label.squeeze()).item()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "b502a0ea-6570-47a6-a7b8-9d0a307d0541",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b502a0ea-6570-47a6-a7b8-9d0a307d0541",
        "outputId": "61dbc5d1-96ed-4b03-c3c7-e3e0771d30a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "train_loss : 0.6070794276893139 val_loss : 0.6617118120193481\n",
            "train_accuracy : 67.5625 val_accuracy : 60.5\n",
            "==================================================\n",
            "Epoch 2\n",
            "train_loss : 0.5303462790325284 val_loss : 0.6197794526815414\n",
            "train_accuracy : 73.3125 val_accuracy : 68.0\n",
            "==================================================\n",
            "Epoch 3\n",
            "train_loss : 0.42751097306609154 val_loss : 0.6882550865411758\n",
            "train_accuracy : 80.1875 val_accuracy : 66.0\n",
            "==================================================\n",
            "Epoch 4\n",
            "train_loss : 0.34107351722195745 val_loss : 0.8736861050128937\n",
            "train_accuracy : 84.9375 val_accuracy : 66.0\n",
            "==================================================\n",
            "Epoch 5\n",
            "train_loss : 0.2371380286058411 val_loss : 0.9725182354450226\n",
            "train_accuracy : 90.8125 val_accuracy : 66.5\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "clip = 5\n",
        "epochs = 5\n",
        "dev_loss_min = np.Inf\n",
        "# train for some number of epochs\n",
        "epoch_tr_loss,epoch_dv_loss = [],[]\n",
        "epoch_tr_acc,epoch_dv_acc = [],[]\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    train_losses = []\n",
        "    train_acc = 0.0\n",
        "    model.train()\n",
        "    # initialize hidden state\n",
        "    h = model.init_hidden(batch_size)\n",
        "    for inputs, labels in train_loader:\n",
        "\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        h = tuple([each.data for each in h])\n",
        "\n",
        "        model.zero_grad()\n",
        "        output,h = model(inputs,h)\n",
        "\n",
        "        # calculate the loss and perform backprop\n",
        "        loss = criterion(output.squeeze(), labels.float())\n",
        "        loss.backward()\n",
        "        train_losses.append(loss.item())\n",
        "        # calculating accuracy\n",
        "        accuracy = acc(output,labels)\n",
        "        train_acc += accuracy\n",
        "        #`clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "\n",
        "    dev_h = model.init_hidden(batch_size)\n",
        "    dev_losses = []\n",
        "    dev_acc = 0.0\n",
        "    model.eval()\n",
        "    for inputs, labels in dev_loader:\n",
        "            dev_h = tuple([each.data for each in dev_h])\n",
        "\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            output, dev_h = model(inputs,dev_h)\n",
        "            val_loss = criterion(output.squeeze(), labels.float())\n",
        "\n",
        "            dev_losses.append(val_loss.item())\n",
        "\n",
        "            accuracy = acc(output,labels)\n",
        "            dev_acc += accuracy\n",
        "\n",
        "    epoch_train_loss = np.mean(train_losses)\n",
        "    epoch_dev_loss = np.mean(dev_losses)\n",
        "    epoch_train_acc = train_acc/len(train_loader.dataset)\n",
        "    epoch_dev_acc = dev_acc/len(dev_loader.dataset)\n",
        "    epoch_tr_loss.append(epoch_train_loss)\n",
        "    epoch_dv_loss.append(epoch_dev_loss)\n",
        "    epoch_tr_acc.append(epoch_train_acc)\n",
        "    epoch_dv_acc.append(epoch_dev_acc)\n",
        "\n",
        "    print(f'Epoch {epoch+1}')\n",
        "    print(f'train_loss : {epoch_train_loss} val_loss : {epoch_dev_loss}')\n",
        "    print(f'train_accuracy : {epoch_train_acc*100} val_accuracy : {epoch_dev_acc*100}')\n",
        "\n",
        "    # if dev_loss goes less than or equal to dev_loss_min then save your model and update the dev_loss_min\n",
        "    if epoch_dev_loss <= dev_loss_min:\n",
        "        torch.save(model.state_dict(), 'sentiment_model.pt')\n",
        "        #print('Dev loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,epoch_dev_loss))\n",
        "        valid_loss_min = epoch_dev_loss\n",
        "    print(25*'==')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa883eea",
      "metadata": {
        "id": "aa883eea"
      },
      "source": [
        "NOTE: your train loss should be smaller than 1 and your train accuracy should be over 75%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "cd036fbe-9b75-4217-add1-20dd33cc48a3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cd036fbe-9b75-4217-add1-20dd33cc48a3",
        "outputId": "b2d0d3be-f154-4453-ead1-a882ead038db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.6650\n",
            "AUROC: 0.7094\n",
            "F1 Score: 0.7179\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "test_h = model.init_hidden(batch_size)\n",
        "test_acc = 0.0\n",
        "\n",
        "# Evaluate model on test data and report the accuracy, AUROC, and F1 score\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        output, _ = model(inputs, test_h)\n",
        "        test_acc += acc(output, labels)\n",
        "\n",
        "test_acc /= len(test_loader.dataset)\n",
        "\n",
        "# Compute AUROC and F1 score\n",
        "predictions = torch.sigmoid(output.squeeze())  # Convert logits to probabilities\n",
        "auroc_score = auroc(predictions, labels, task=\"binary\")\n",
        "f1_value = f1_score(predictions > 0.5, labels, task=\"binary\")  # Assuming threshold of 0.5 for F1 score\n",
        "\n",
        "\n",
        "print(f'Test Accuracy: {test_acc:.4f}')\n",
        "print(f'AUROC: {auroc_score:.4f}')\n",
        "print(f'F1 Score: {f1_value:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f332ea53",
      "metadata": {
        "id": "f332ea53"
      },
      "source": [
        "NOTE: your eval accuracy should be of at least 60%."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8917a512",
      "metadata": {
        "id": "8917a512"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
